{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2158f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# NSMC 훈련용 코퍼스 내 문장 및 라벨 데이터 추출\n",
    "\n",
    "f_train = pd.read_csv('맛집리뷰text.txt', sep='\\t')\n",
    "train_pair = [(row[1], row[2]) for _, row in f_train.iterrows() if type(row[1]) == str]\n",
    "\n",
    "train_data  = [pair[0] for pair in train_pair]\n",
    "train_label = [pair[1] for pair in train_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3761fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 가성비 최고에 맛까지 좋은 노포 식당입니다..\n",
      "라벨: 1\n",
      "\n",
      "문장: 보통의 닭곰탕집들처럼 노계나 폐닭을 사용하지 않고 일반 육계를 사용해서 잡내없고 육질이 참 부드럽습니다..\n",
      "라벨: 1\n",
      "\n",
      "문장: 멸치육수베이스의 칼국수도 딱 멸치육수의 교과서같은 맛입니다..\n",
      "라벨: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 추출된 문장 및 라벨 데이터 일부 확인\n",
    "\n",
    "for data, label in zip(train_data[:3], train_label[:3]):\n",
    "    print(f'문장: {data}\\n라벨: {label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309e2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 전반에 사용될 변수 사전 정의\n",
    "\n",
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'num_epoch': 15,\n",
    "    'lr': 0.003,\n",
    "    'dropout': 0.5,\n",
    "    'min_frequency': 3,\n",
    "    'max_len': 20,\n",
    "    \n",
    "    'vocab_size': 20000,\n",
    "    'embed_dim': 100,\n",
    "    'hidden_dim': 256,\n",
    "    'filter_sizes': [2, 3, 4],\n",
    "    'num_filters': 100,\n",
    "    'output_dim': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e6bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출한 문장 데이터 텍스트 파일로 저장\n",
    "\n",
    "with open('train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in train_data:\n",
    "        print(line, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1783fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# BertWordPieceTokenizer 초기화\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "# 앞서 제작한 텍스트 파일 활용해 토크나이저 훈련\n",
    "\n",
    "trainer = tokenizer.train(\n",
    "    files=['train_tokenizer.txt'],\n",
    "    vocab_size=params['vocab_size'],\n",
    "    min_frequency=params['min_frequency'],\n",
    "    special_tokens=['[PAD]', '[SOS]', '[EOS]', '[UNK]'],\n",
    "    wordpieces_prefix=\"##\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa393f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'BertWordPiece',\n",
       " 'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'mask_token': '[MASK]',\n",
       " 'clean_text': True,\n",
       " 'handle_chinese_chars': True,\n",
       " 'strip_accents': None,\n",
       " 'lowercase': True,\n",
       " 'wordpieces_prefix': '##'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저 파라미터 확인\n",
    "\n",
    "tokenizer._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806a6bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 패딩 토큰 인덱스 확인\n",
    "\n",
    "print(tokenizer.token_to_id('[PAD]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a170119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스페셜 토큰 변수화\n",
    "\n",
    "pad_idx = tokenizer.token_to_id('[PAD]')\n",
    "sos_idx = tokenizer.token_to_id('[SOS]')\n",
    "eos_idx = tokenizer.token_to_id('[EOS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "464027b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저에 대해 패딩 옵션 설정\n",
    "tokenizer.enable_padding(pad_id=pad_idx, pad_token='[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fde840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'encode_batch' 함수 이용해 훈련 데이터셋에 대해 토크나이즈 수행\n",
    "\n",
    "encoded_data = tokenizer.encode_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76cca161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터:\t39 개\n",
      "훈련 라벨:\t39 개\n",
      "인코딩 데이터:\t39 개\n"
     ]
    }
   ],
   "source": [
    "# 데이터 개수 확인\n",
    "\n",
    "print(f'훈련 데이터:\\t{len(train_data)} 개')\n",
    "print(f'훈련 라벨:\\t{len(train_label)} 개')\n",
    "print(f'인코딩 데이터:\\t{len(encoded_data)} 개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b526be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰: ['혼', '##ᄇ', '##ᅡᆸ', '##하', '##기', '좋아요', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "아이디: [336, 104, 292, 168, 180, 271, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 초기 훈련 결과 확인\n",
    "\n",
    "print(f'토큰: {encoded_data[20].tokens}\\n')\n",
    "print(f'아이디: {encoded_data[20].ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66dba2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리 함수 정의\n",
    "\n",
    "def postprocess(input_ids):\n",
    "    input_ids = [sos_idx] + input_ids\n",
    "    \n",
    "    # 문장 최대 길이까지 슬라이싱\n",
    "    input_ids = input_ids[:params['max_len']]\n",
    "\n",
    "    # 패딩 토큰이 포함된 문장이라면 원 문장 말미에 <EOS> 토큰 삽입 \n",
    "    if pad_idx in input_ids:\n",
    "        pad_start = input_ids.index(pad_idx)\n",
    "        input_ids[pad_start] = eos_idx\n",
    "\n",
    "    # 패딩 토큰이 포함되지 않은 문장이라면, 시퀀스 말미에 <EOS> 토큰 삽입\n",
    "    else:\n",
    "        input_ids[-1] = eos_idx\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5787f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리 함수 이용해 토크나이즈 문장 후처리\n",
    "\n",
    "processed_data = [postprocess(data.ids) for data in encoded_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b27281f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "후처리 결과: [1, 336, 104, 292, 168, 180, 271, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "후처리 결과 디코딩: 혼밥하기 좋아요\n"
     ]
    }
   ],
   "source": [
    "# 후처리 결과 확인\n",
    "\n",
    "print(f'후처리 결과: {processed_data[20]}\\n')\n",
    "print(f'후처리 결과 디코딩: {tokenizer.decode(processed_data[20])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e14e8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "torch.manual_seed(32)\n",
    "torch.cuda.manual_seed(32)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e881f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(params['vocab_size'], params['embed_dim'], padding_idx=pad_idx)\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, \n",
    "                                              out_channels=params['num_filters'], \n",
    "                                              kernel_size=(fs, params['embed_dim'])) \n",
    "                                    for fs in params['filter_sizes']])\n",
    "        \n",
    "        self.fc = nn.Linear(len(params['filter_sizes']) * params['num_filters'], 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids    = [배치 사이즈, 문장 길이]\n",
    "\n",
    "        embedded = self.embedding(input_ids).unsqueeze(1)\n",
    "        # embedded     = [배치 사이즈, 채널 개수, 임베딩 차원]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        # conved_n     = [배치 사이즈, 필터 개수, 문장 길이 - 필터 리스트[n] + 1]\n",
    "        \n",
    "        max_pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # max_pooled_n = [배치 사이즈, 필터 개수]\n",
    "\n",
    "        cat = self.dropout(torch.cat(max_pooled, dim = 1))\n",
    "        # cat          = [배치 사이즈, 필터 개수 x len(필터 리스트)]\n",
    "\n",
    "        return self.fc(cat)  # [배치 사이즈, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5392c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 및 라벨 데이터 torch Tensor로 변환\n",
    "\n",
    "processed_data = [torch.LongTensor(data).to(device) for data in processed_data]\n",
    "train_label = [torch.FloatTensor([label]).to(device) for label in train_label]\n",
    "\n",
    "\n",
    "# torch Tensor로 변환한 데이터 이용해 Iterator 정의\n",
    "\n",
    "train_iter = data.DataLoader(processed_data, batch_size=params['batch_size'])\n",
    "label_iter = data.DataLoader(train_label, batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5756d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.814\n",
      "Epoch: 02 | Train Loss: 0.460\n",
      "Epoch: 03 | Train Loss: 0.334\n",
      "Epoch: 04 | Train Loss: 0.222\n",
      "Epoch: 05 | Train Loss: 0.142\n",
      "Epoch: 06 | Train Loss: 0.076\n",
      "Epoch: 07 | Train Loss: 0.059\n",
      "Epoch: 08 | Train Loss: 0.076\n",
      "Epoch: 09 | Train Loss: 0.054\n",
      "Epoch: 10 | Train Loss: 0.029\n",
      "Epoch: 11 | Train Loss: 0.022\n",
      "Epoch: 12 | Train Loss: 0.012\n",
      "Epoch: 13 | Train Loss: 0.014\n",
      "Epoch: 14 | Train Loss: 0.015\n",
      "Epoch: 15 | Train Loss: 0.019\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "for epoch in range(params['num_epoch']):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for (batch, label) in zip(train_iter, label_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(batch).squeeze(1)    # [배치 사이즈]\n",
    "        labels = label.view(label.size(0))  # [배치 사이즈]\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = epoch_loss / len(train_iter)        \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "025630f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 해석에 필요한 Captum API 임포트 및 정의\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=pad_idx)  # 레퍼런스 생성을 위한 모듈\n",
    "lig = LayerIntegratedGradients(model, model.embedding)             # 결과 해석에 사용되는 IntegratedGradient 기법 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fa10e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data_records_ig = []\n",
    "\n",
    "label_vocab  = {0: '부정', 1: '긍정'}\n",
    "\n",
    "def interpret_sentence(model, sentence, label = 0):       \n",
    "    model.zero_grad()\n",
    "    \n",
    "    input_ids = tokenizer.encode(sentence)\n",
    "    input_tokens = input_ids.tokens[:params['max_len']]\n",
    "    \n",
    "    input_ids = postprocess(input_ids.ids)\n",
    "    input_indices_tensor = torch.LongTensor(input_ids).to(device).unsqueeze(0)\n",
    "\n",
    "    # 단일 문장에 대한 예측 작업 수행\n",
    "    pred = torch.sigmoid(model(input_indices_tensor)).item()\n",
    "    pred_ind = round(pred)\n",
    "\n",
    "    # 베이스 라인 역할을 할 Reference 생성: 주로 패딩 토큰으로 채워줌\n",
    "    reference_indices = token_reference.generate_reference(params['max_len'], device=device).unsqueeze(0)\n",
    "\n",
    "    # LayerIntegratedGradients 모듈 활용해 개별 단어의 속성값 및 델타값 근사치 계산\n",
    "    attributions_ig, delta = lig.attribute(input_indices_tensor, reference_indices, n_steps=500, return_convergence_delta=True)\n",
    "\n",
    "    print('pred: ', label_vocab[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n",
    "    \n",
    "    add_attributions_to_visualizer(attributions_ig, input_tokens, )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "\n",
    "    # 시각화 위해 샘플을 리스트에 추가\n",
    "    vis_data_records.append(visualization.\n",
    "                                VisualizationDataRecord(\n",
    "                                    attributions,\n",
    "                                    pred,\n",
    "                                    label_vocab[pred_ind],\n",
    "                                    label_vocab[label],\n",
    "                                    label_vocab[1],\n",
    "                                    attributions.sum(),       \n",
    "                                    text,\n",
    "                                delta\n",
    "                                )\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fb7bffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  긍정 ( 0.92 ) , delta:  tensor([0.9930], dtype=torch.float64)\n",
      "pred:  부정 ( 0.11 ) , delta:  tensor([0.3498], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 예제 문장 추가 및 분석 수행\n",
    "interpret_sentence(model, '맛있어서 또 방문할 것 같아요. 국물이 일품입니다', label=1)\n",
    "interpret_sentence(model, '가게도 낡았고 진짜 별로.. 사장님도 불친절하고 기분만 나빴네요', label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7835f5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시각화 결과 표 변환\n",
    "visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5095e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bee40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
